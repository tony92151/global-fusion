[general]
tbpath=./tblogs_shakespeare/shakespeare_lstm_fedavg
nodes=50

[dgc] # deep-gradient-compression
dgc=False
compress_ratio = 1.0
#[0.25,0.0225,0.015625,0.004,0.001]
momentum = 0.9
momentum_correction=True

[gf] # global fusion
global_fusion=False
fusing_ratio=[0.0,0.04,0.078,0.113,0.143,0.168,0.186,0.197]
# [0.0,0.04,0.078,0.113,0.143,0.168,0.186,0.197]
global_fusion_after_warmup=True

[trainer]
# [lstm_shakespeare]
model=lstm_shakespeare
max_iteration=80
device=GPU
dataset_path=./data/shakespeare
dataset_type=niid
# batchsize in local client
local_bs=16
# ep in local client
local_ep=1
frac=0.1

# warmup learning loss
start_lr=0.0001
max_lr=0.0004
min_lr=0.0000001
base_step=10
end_step=75


optimizer=GFDGCSGD
lossfun=crossentropy

[aggregator]
# ["SGD", "ADAGRAD", "ADAM", "YOGI"]
optimizer=SGD

[eval]
output=result.jpg
title=BCFL(niid)
