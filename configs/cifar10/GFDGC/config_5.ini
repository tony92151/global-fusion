[general]
# tensorboard, wandb
toolkit="tensorboard"
logdir=./logs/cifar_r56_gfdgc_cr0.5_ADAM
nodes=20
frac=1.0

###########################################################################
###########################################################################
[compression]
# dgc, sgc, gfdgc, gfgc
algorithm = gfdgc


[gfdgc] # global fusion deep-gradient-compression
compress_rate = 0.5
momentum = 0.9
# gf
fusing_ratio = [0.0,0.04,0.078,0.113,0.143,0.168,0.186,0.197]
# [0.0,0.04,0.078,0.113,0.143,0.168,0.186,0.197]
global_momentum = 0.9

###########################################################################
###########################################################################

[trainer]
# cifar10
# [small_cifar, resnet18_cifar, resnet50_cifar, resnet101_cifar, resnet56_cifar_gdc, resnet110_cifar_gdc]
model=resnet56_cifar_gdc
max_iteration=220
device=GPU
# cifar10, shakespeare
dataset=cifar10
dataset_path=./data/cifar10
# batchsize in local client
local_bs=128
# ep in local client
local_ep=1

# warmup learning loss
start_lr=0.001
# if aggregated optimizer is ADAM, max_lr=0.004 is better in Cifar10
max_lr=0.004
min_lr=0.00001
base_step=10
end_step=215


optimizer=SGD
lossfun=crossentropy
# SGD: dgc setting {"momentum": 0.9,"nesterov": True,"weight_decay": 1e-4}
optimizer_args = {"momentum": 0.9,"nesterov": True,"weight_decay": 1e-4}

[aggregator]
# ["SGD", "ADAGRAD", "ADAM", "YOGI"]
optimizer=ADAM
# SGD: default {"momentum": 0,"nesterov": False,"weight_decay": 0}
# optimizer_args =  {"momentum": 0,"nesterov": False,"weight_decay": 0}

# ADAM: default {"betas":(0.9, 0.999), "eps":1e-08, "weight_decay":0, "amsgrad":False}
optimizer_args =  {"betas":(0.9, 0.999), "eps":1e-3, "weight_decay":0, "amsgrad":False}

# YOGI: default {"betas":(0.9, 0.999), "eps":1e-08, "weight_decay":0, "amsgrad":False}
# optimizer_args =  {"betas": (0.9, 0.999),"eps": 1e-3,"initial_accumulator": 1e-6, "weight_decay": 0}

[eval]
output=result.jpg
title=BCFL(niid)
